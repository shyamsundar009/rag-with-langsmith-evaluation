{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import Custom_Rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "F=Custom_Rag(model=\"gpt-4o\")\n",
    "F.main(\"Who is shyam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=Custom_Rag(model=\"meta/llama3-70b-instruct\")\n",
    "l.main(\"Who is shyam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=Custom_Rag(model=\"gemma-7b-it\")\n",
    "ll.main(\"Who is shyam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=Custom_Rag(model=\"mixtral-8x7b-32768\")\n",
    "k.main(\"Who is shyam?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION METRICS - LLM JUDGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](rag_eval_overview-c0c3c2ac03f44b2b67c58ab67f32d82a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs.get(\"prompt\")\n",
    "    reference = example.outputs.get(\"answer\")\n",
    "    prediction = run.outputs.get(\"response\")\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs.get(\"prompt\")\n",
    "    prediction = run.outputs.get(\"response\")\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}\n",
    "\n",
    "# Prompt\n",
    "grade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs.get(\"prompt\")\n",
    "    contexts = run.outputs.get(\"context\")\n",
    "\n",
    "    # RAG answer\n",
    "    prediction = run.outputs.get(\"response\")\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs.get(\"prompt\")\n",
    "    contexts = run.outputs.get(\"context\")\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gpt-4o rag eval-7f707e27' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=add979f1-1dc8-4b8d-9b29-1b77c64ce5ef\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000002002B32C2F0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002002B345340> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000002002B32DE80> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002002B5AD850> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000002002B32F800> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002002B5AE870> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "3it [01:07, 22.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import  evaluate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"raft-custom-dataset\"\n",
    "experiment_results = evaluate(\n",
    "    lambda inputs: Custom_Rag(model=\"gpt-4o\").main(inputs[\"prompt\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator, answer_helpfulness_evaluator, answer_hallucination_evaluator, docs_relevance_evaluator],\n",
    "    experiment_prefix=\"gpt-4o rag eval\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-4-turbo\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models=[\"gpt-3.5-turbo\", \"gpt-4o\",\"gemma-7b-it\", \"mixtral-8x7b-32768\", \"llama3-8b-8192\",\"databricks/dbrx-instruct\", \"microsoft/phi-3-small-8k-instruct\", \"google/gemma-7b\", \"meta/llama3-70b-instruct\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gpt-3.5-turbo rag eval-1e65d1b8' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=c4b059ed-af4b-47bc-9e43-61f0241e7014\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x00000200016E23F0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000200016F9E20> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''client=<openai.resources.chat.completions.Completions object at 0x00000200083E76E0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000200016FAC90> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200083E6E70> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000200016FBBF0> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200016E37A0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020001707050> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000002000843B3B0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002000171D850> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200016F8D10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002000171EE40> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200016F21B0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020001740410> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200016F0AD0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020001741700> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200016F38F0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000200017427E0> temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:17,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gpt-4o rag eval-1e6b6d4c' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=891de551-c8a6-4bd3-b4b3-77c88edba0e2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x0000020006637C50> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002000186D340> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200084002C0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020006623440> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020002C6B920> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002000663CB90> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020001901D00> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002000663FB90> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x00000200083BDCA0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020006685E80> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020002A58470> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020006686F00> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020001777E60> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020006687F20> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020006635F70> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000200066B4FB0> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020006637140> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000200066221E0> model_name='gpt-4o' temperature=0.2 openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:17,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gemma-7b-it rag eval-52c284b7' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=7f6bf2c3-10da-4800-9c70-34e2bb893e24\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001874950> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001707A10> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x000002000678C530> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001704B00> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001743050> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200017077A0> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001877DA0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001707A70> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001742570> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200083B3FB0> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001707920> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200083B2660> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001741D90> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200083B01A0> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x00000200017408C0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200083B0620> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001741220> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200083B0980> model_name='gemma-7b-it' temperature=0.2 groq_api_key=SecretStr('**********')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:25,  9.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'mixtral-8x7b-32768 rag eval-b508d235' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=b5506f9c-4667-4b31-8393-9dacb5f4185c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x0000020002833290> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020002B08D70> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020002830860> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020002ABB080> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x00000200064D3FE0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020002891A00> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020001A498E0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200017A5D90> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020006637410> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001887890> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x000002002B303C80> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020064A41E50> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x000002000843A6F0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020064A431D0> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020002B08800> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020064A421B0> temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x000002000195DE50> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020064A43800> temperature=0.2 groq_api_key=SecretStr('**********')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:56, 12.90s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'llama3-8b-8192 rag eval-f4560e0e' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=ff43c011-939a-4a27-8da1-52aad8b9d409\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x0000020002AB9910> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200083871A0> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020002A50740> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001A83AD0> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x00000200067FF620> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001A80B30> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020002A522A0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020001A838F0> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x00000200017A7E90> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200068EFFE0> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x00000200017A4CB0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000200068EF320> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x000002000196D9A0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020002C685C0> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x000002000196CB00> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002002B32C140> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n",
      "client=<groq.resources.chat.completions.Completions object at 0x0000020008387FE0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002002B32D220> model_name='llama3-8b-8192' temperature=0.2 groq_api_key=SecretStr('**********')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:45, 11.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'databricks/dbrx-instruct rag eval-2b9c33f9' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=2b0b2397-7fd4-4591-8502-c09cbd61a86d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n",
      "model='databricks/dbrx-instruct' temperature=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:21,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'microsoft/phi-3-small-8k-instruct rag eval-cc638703' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=94484226-c3ea-40f2-97bd-357858e42eee\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n",
      "model='microsoft/phi-3-small-8k-instruct' temperature=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:56, 12.99s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'google/gemma-7b rag eval-f997e428' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=09a3b561-8af1-4051-998c-643bf54e3995\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n",
      "model='google/gemma-7b' temperature=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:31, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'meta/llama3-70b-instruct rag eval-ec45d1f6' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/f7abf090-86e8-4960-94f8-0fb9c3569c53/compare?selectedSessions=1fc5596d-35af-48f0-9df6-e48c787ee512\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n",
      "model='meta/llama3-70b-instruct' temperature=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "9it [01:26,  9.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in list_of_models:\n",
    "    xperiment_results = evaluate(\n",
    "    lambda inputs: Custom_Rag(model=model).main(inputs[\"prompt\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator, answer_helpfulness_evaluator, answer_hallucination_evaluator, docs_relevance_evaluator],\n",
    "    experiment_prefix=f\"{model} rag eval\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-4-turbo\",\n",
    "    },\n",
    "    num_repetitions=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
