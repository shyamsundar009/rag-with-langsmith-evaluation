{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, wrappers\n",
    "from openai import Client\n",
    "\n",
    "openai = wrappers.wrap_openai(Client())\n",
    "\n",
    "@traceable\n",
    "def label_text(text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    result = openai.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-3.5-turbo\", temperature=0\n",
    "    )\n",
    "    return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "[Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\utils.py:121\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m      6\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShut up, idiot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToxic\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre a wonderful person\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot toxic\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is unacceptable. I want to speak to the manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot toxic\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToxic Queries\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m inputs, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m*\u001b[39m[({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: label}) \u001b[38;5;28;01mfor\u001b[39;00m text, label \u001b[38;5;129;01min\u001b[39;00m examples]\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m client\u001b[38;5;241m.\u001b[39mcreate_examples(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs, dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid)\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\client.py:2368\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[1;34m(self, dataset_name, description, data_type)\u001b[0m\n\u001b[0;32m   2358\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ls_schemas\u001b[38;5;241m.\u001b[39mDatasetCreate(\n\u001b[0;32m   2359\u001b[0m     name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   2360\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m   2361\u001b[0m     data_type\u001b[38;5;241m=\u001b[39mdata_type,\n\u001b[0;32m   2362\u001b[0m )\n\u001b[0;32m   2363\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m   2364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2365\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   2366\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   2367\u001b[0m )\n\u001b[1;32m-> 2368\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[0;32m   2370\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   2371\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[0;32m   2372\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[0;32m   2373\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\utils.py:123\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    121\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    (\"Shut up, idiot\", \"Toxic\"),\n",
    "    (\"You're a wonderful person\", \"Not toxic\"),\n",
    "    (\"This is the worst thing ever\", \"Toxic\"),\n",
    "    (\"I had a great day today\", \"Not toxic\"),\n",
    "    (\"Nobody likes you\", \"Toxic\"),\n",
    "    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),\n",
    "]\n",
    "\n",
    "dataset_name = \"Toxic Queries\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "inputs, outputs = zip(\n",
    "    *[({\"text\": text}, {\"label\": label}) for text, label in examples]\n",
    ")\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def correct_label(root_run: Run, example: Example) -> dict:\n",
    "    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")\n",
    "    return {\"score\": int(score), \"key\": \"correct_label\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Toxic Queries-f9fac829' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/beb87acc-ecc1-479d-9c75-7820debf98a7/compare?selectedSessions=21be7e58-c913-4f45-a522-c4dbd0d6671d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:04,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "# from app import label_text\n",
    "dataset_name = \"Toxic Queries\"\n",
    "\n",
    "results = evaluate(\n",
    "    lambda inputs: label_text(inputs[\"text\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    experiment_prefix=\"Toxic Queries\",\n",
    "    description=\"Testing the baseline system.\",  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Toxic Queries with repeatation-edd2abcc' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/beb87acc-ecc1-479d-9c75-7820debf98a7/compare?selectedSessions=0fff2793-9ae3-44a1-be24-8c6656a32f88\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:03,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "# from app import label_text\n",
    "dataset_name = \"Toxic Queries\"\n",
    "\n",
    "results = evaluate(\n",
    "    lambda inputs: label_text(inputs[\"text\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    experiment_prefix=\"Toxic Queries with repeatation\",\n",
    "    description=\"Testing the baseline system with three responses\",  # optional\n",
    "    num_repetitions=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def summary_eval(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    correct = 0\n",
    "    for i, run in enumerate(runs):\n",
    "        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
    "            correct += 1\n",
    "    if correct / len(runs) > 0.5:\n",
    "        return {\"key\": \"pass\", \"score\": True}\n",
    "    else:\n",
    "        return {\"key\": \"pass\", \"score\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Toxic Queries with summary and toxix label with three repatations-85c6afc2' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/ebd5b144-7f60-4109-b36c-b0fe8f5a2cca/compare?selectedSessions=a094f573-88fd-44d6-81b4-4fa40bce1c10\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run 6cfc8af2-dcd0-4f02-a89a-2000e7e13918: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "1it [00:05,  5.48s/it]Error running evaluator <DynamicRunEvaluator summary_eval> on run 152592b2-bfc9-4a8b-9eea-284d4e4b9518: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run c1bb9a6e-bc74-45af-90dc-15f5d9d304a0: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "3it [00:05,  1.46s/it]Error running evaluator <DynamicRunEvaluator summary_eval> on run 1a70c880-faf1-43c3-b8d4-8ae13152080a: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run 6cb31141-cf6b-4394-8e45-063888e1b778: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run 7975e4d5-1197-46c8-8a26-c9726916e997: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "6it [00:05,  1.71it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run f761d1ee-1365-436f-9a2b-fa4518e4b29e: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run f274a20e-e77e-46ca-b2fd-815c4bc7aff2: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "8it [00:06,  2.33it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run 187cbc8a-a7d1-46a7-8f56-b7bb7f01ddde: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run e4f2fbe1-37ef-42a1-ac91-548d2075ad18: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run 25aaba09-509d-4197-8cb4-264a80525cea: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run 3cb5e27c-ce95-44dc-a7c5-c1438c6f4ceb: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "12it [00:06,  3.63it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run cd8fe806-65a8-4eca-8996-cb8fe96b95ed: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run dd44c83c-1b63-42b1-97c7-cda413643bfc: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "14it [00:07,  3.05it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run 1a8b1b80-fd1f-4259-a533-e4926d52affb: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "15it [00:07,  3.08it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run b6b21eec-5a75-403e-bf6f-7cef5dfd0dbd: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "16it [00:08,  3.12it/s]Error running evaluator <DynamicRunEvaluator summary_eval> on run a66c6a80-2e29-47fb-b0c1-fb86e39639e3: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator summary_eval> on run 4f2307e0-87e3-4c87-a730-3ca59ee668b6: AttributeError(\"'tuple' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1231, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\shyams\\Downloads\\projects\\rag with langsmith evaluation\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shyams\\AppData\\Local\\Temp\\ipykernel_3632\\3829193764.py\", line 6, in summary_eval\n",
      "    if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
      "       ^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'outputs'\n",
      "18it [00:08,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(\n",
    "    lambda inputs: label_text(inputs[\"text\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    experiment_prefix=\"Toxic Queries with summary and toxix label with three repatations\",\n",
    "    num_repetitions=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),\n",
    "  (\"user\", \"{text}\")\n",
    "])\n",
    "chat_model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Toxic Queries-3d274b3f' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/ebd5b144-7f60-4109-b36c-b0fe8f5a2cca/compare?selectedSessions=e7714f5d-bdc0-4507-b9a0-e528a9cbc1a6\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:03,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "results = evaluate(\n",
    "    chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    experiment_prefix=\"Toxic Queries\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import wikipedia as wp\n",
    "\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable\n",
    "def generate_wiki_search(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Generate a search query to pass into wikipedia to answer the user's question. Return only the search query and nothing more. This will passed in directly to the wikipedia search engine.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    result = openai.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retrieve(query):\n",
    "    results = []\n",
    "    for term in wp.search(query, results = 10):\n",
    "        try:\n",
    "            page = wp.page(term, auto_suggest=False)\n",
    "            results.append({\n",
    "                \"page_content\": page.summary,\n",
    "                \"type\": \"Document\",\n",
    "                \"metadata\": {\"url\": page.url}\n",
    "            })\n",
    "        except wp.DisambiguationError:\n",
    "            pass\n",
    "        if len(results) >= 2:\n",
    "            return results\n",
    "\n",
    "@traceable\n",
    "def generate_answer(question, context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"Answer the user's question based ONLY on the content below:\\n\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    result = openai.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def rag_pipeline(question):\n",
    "    query = generate_wiki_search(question)\n",
    "    context = \"\\n\\n\".join([doc[\"page_content\"] for doc in retrieve(query)])\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "    (\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models.\"),\n",
    "    (\"What is LangSmith?\", \"LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc.\")\n",
    "]\n",
    "\n",
    "dataset_name = \"Wikipedia RAG\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "def document_relevance(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A very simple evaluator that checks to see if the input of the retrieval step exists\n",
    "    in the retrieved docs.\n",
    "    \"\"\"\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"rag_pipeline\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve\")\n",
    "    page_contents = \"\\n\\n\".join(doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"])\n",
    "    score = retrieve_run.inputs[\"query\"] in page_contents\n",
    "    return {\"key\": \"simple_document_relevance\", \"score\": score}\n",
    "\n",
    "def hallucination(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"rag_pipeline\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve\")\n",
    "    page_contents = \"\\n\\n\".join(doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"])\n",
    "    generation = rag_pipeline_run.outputs[\"output\"]\n",
    "\n",
    "    # Data model\n",
    "    class GradeHallucinations(BaseModel):\n",
    "        \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "        binary_score: int = Field(description=\"Answer is grounded in the facts, 1 or 0\")\n",
    "\n",
    "    # LLM with function call\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "    structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "    # Prompt\n",
    "    system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "        Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "    hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "    score = hallucination_grader.invoke({\"documents\": page_contents, \"generation\": generation})\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": int(score.binary_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-wiki-oai-81f3e1b2' at:\n",
      "https://smith.langchain.com/o/b47753b3-6d84-5fa3-b240-64adbbe1c7be/datasets/b52af8f3-5d72-40d3-b961-68bce46a5ea8/compare?selectedSessions=8a1f9b78-0373-48d0-8769-dc03e0938864\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:13,  6.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    lambda inputs: rag_pipeline(inputs[\"input\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[document_relevance, hallucination],\n",
    "    experiment_prefix=\"rag-wiki-oai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
